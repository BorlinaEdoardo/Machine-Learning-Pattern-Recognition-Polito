{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b71c4e",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f274ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as linalg\n",
    "import scipy.optimize as optimize\n",
    "import sklearn as sk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "328acc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "\n",
    "def vcol(v):\n",
    "    l = np.array(v).shape[0]\n",
    "    return np.reshape(np.array(v), [l, 1])\n",
    "\n",
    "def vrow(v):\n",
    "    l = np.array(v).shape[0]\n",
    "    return np.reshape(np.array(v), [1, l])\n",
    "\n",
    "def random_z(n):\n",
    "    return [random.choice([1, -1]) for _ in range(n)]\n",
    "\n",
    "def random_x(n):\n",
    "    return [random.randint(0,9) for _ in range(n)]\n",
    "\n",
    "def load_iris_binary():\n",
    "    D, L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']    \n",
    "    D = D[:, L != 0] # We remove setosa from D\n",
    "    L = L[L!=0] # We remove setosa from L\n",
    "    L[L==2] = 0 # We assign label 0 to virginica (was label 2)\n",
    "    return D, L\n",
    "\n",
    "\n",
    "def split_db_2to1(D, L, seed=0):\n",
    "    \n",
    "    nTrain = int(D.shape[1]*2.0/3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    \n",
    "    DTR = D[:, idxTrain]\n",
    "    DVAL = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LVAL = L[idxTest]\n",
    "    return (DTR, LTR), (DVAL, LVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "29fd0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def compute_D(x, K = 1):\n",
    "    l = np.array(x).shape[1]\n",
    "    D = np.vstack([x, np.ones(l)*K])\n",
    "\n",
    "    return D\n",
    "\n",
    "def compute_H(x, z, K = 1): # note!!! x is a vector\n",
    "    D = compute_D(x, K)\n",
    "    G = D.T @ D\n",
    "    zv = vrow(z)\n",
    "    H = (zv.T @ zv) * G\n",
    "    return H\n",
    "def find_L(alpha, H):\n",
    "    Ha = H @ vcol(alpha)\n",
    "    return 0.5 * (vrow(alpha) @ Ha).ravel() - alpha.sum()\n",
    "\n",
    "def find_grad_L(alpha, H):\n",
    "    # grad L = H @ alpha - 1\n",
    "    alpha = np.ravel(alpha)\n",
    "    return H @ alpha - np.ones_like(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1d6f3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize SVM\n",
    "\n",
    "def svm_calc_w_cap(x, z, K = 1, C = 10):\n",
    "    D = compute_D(x, K)\n",
    "    H = compute_H(x, z, K)\n",
    "    a0 = np.ones(x.shape[1])*0\n",
    "    bounds = [(0, C) for _ in z ]\n",
    "    alpha_opt, _, _ = optimize.fmin_l_bfgs_b(\n",
    "        func = find_L,\n",
    "        x0 = a0,\n",
    "        fprime = find_grad_L,\n",
    "        args = (H,),\n",
    "        bounds=bounds,\n",
    "        pgtol=1e-5,\n",
    "        factr=np.nan\n",
    "    )\n",
    "    w_star_cap = vcol((alpha_opt*z*D).sum(1))\n",
    "    return alpha_opt, w_star_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# primal loss\n",
    "def primal_loss(w_cap, x, z, C):\n",
    "    D = compute_D(x)\n",
    "    # Calcola i margini\n",
    "    margins = z * (w_cap.T @ D)  # z_i * (w^T * x_i + b)\n",
    "    # Hinge loss: max(0, 1 - margin)\n",
    "    hinge_loss = np.maximum(0, 1 - margins)\n",
    "    # Primal loss\n",
    "    return 0.5 * np.linalg.norm(w_cap)**2 + C * np.sum(hinge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0cabccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate is 2.941176470588236%\n"
     ]
    }
   ],
   "source": [
    "(D, L) = load_iris_binary()\n",
    "(x, L_train), (x_test, L_test) = split_db_2to1(D, L)\n",
    "z = L_train * 2 - 1 \n",
    "z_test = L_test * 2 - 1\n",
    "\n",
    "C = 0.1\n",
    "\n",
    "\n",
    "alpha, w_cap = svm_calc_w_cap(x, z, C=C)\n",
    "\n",
    "scores = w_cap.T @ compute_D(x_test) # scores\n",
    "\n",
    "z_predicted = np.where(scores < 0, 0, 1).ravel()\n",
    "comparison = z_predicted == L_test\n",
    "acc = comparison.sum()/len(z_test)\n",
    "print(f\"Error rate is {(1-acc)*100}%\")\n",
    "# Note: accuracy is not a good metric, \n",
    "# but I'm using it just to get an idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fbc727ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute detection cost function (bayesian risk)\n",
    "def detection_cost_function(confusion_matrix, pi, Cfn, Cfp):\n",
    "    false_negative = confusion_matrix[1, 0]\n",
    "    true_negative = confusion_matrix[0, 0]\n",
    "    false_positive = confusion_matrix[0, 1]\n",
    "    true_positive = confusion_matrix[1, 1]\n",
    "    false_negative_rate = false_negative / (false_negative + true_positive)\n",
    "    false_positive_rate = false_positive / (false_positive + true_negative)\n",
    "\n",
    "    return pi*Cfn*false_negative_rate + (1 - pi)*Cfp*false_positive_rate\n",
    "\n",
    "def compute_normalized_dcf(predicted_labels, actual_labels, pi, Cfn, Cfp):\n",
    "    \n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(actual_labels, predicted_labels)\n",
    "    DCF = detection_cost_function(confusion_matrix, pi, Cfn, Cfp)/np.min([pi*Cfn, (1-pi)*Cfp])\n",
    "    return DCF\n",
    "\n",
    "def compute_dcf(predicted_labels, actual_labels, pi, Cfn, Cfp):\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(actual_labels, predicted_labels)\n",
    "    return detection_cost_function(confusion_matrix, pi, Cfn, Cfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9ebfd2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual Loss: [3.77497382]\n",
      "Primal Loss: 3.774973824823995\n",
      "Duality gap: [-4.4408921e-15]\n",
      "0.0625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "zero = np.zeros(w_cap.shape[1])\n",
    "\n",
    "primal = primal_loss(w_cap, x, z, C)\n",
    "dual = -find_L(alpha, compute_H(x, z))\n",
    "print(f\"Dual Loss: {dual}\")\n",
    "print(f\"Primal Loss: {primal}\")\n",
    "\n",
    "print(f\"Duality gap: {primal - dual}\")\n",
    "\n",
    "pi = 0.5\n",
    "\n",
    "DCF = compute_normalized_dcf(z_predicted, L_test, pi, 1, 1)\n",
    "\n",
    "print(DCF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe0ea0",
   "metadata": {},
   "source": [
    "# SVM: Teoria Completa (AI made)\n",
    "\n",
    "## 1. Introduzione alle SVM\n",
    "\n",
    "Le **Support Vector Machines** sono classificatori lineari che cercano un iperpiano di separazione tra due classi. L'idea è trovare il margine massimo tra le classi, che produce una buona generalizzazione.\n",
    "\n",
    "### Notazione\n",
    "- $x_i \\in \\mathbb{R}^d$: vettore di features (dati)\n",
    "- $z_i \\in \\{-1, +1\\}$: etichetta (label)\n",
    "- $w \\in \\mathbb{R}^d$: pesi dell'iperpiano\n",
    "- $b$: bias (termine noto)\n",
    "- $C > 0$: parametro di regolarizzazione (soft margin)\n",
    "- $\\xi_i \\geq 0$: variabili slack (permettono violazioni del vincolo)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Il Problema Primale (Primal Problem)\n",
    "\n",
    "Il problema **primale** è la formulazione originale:\n",
    "\n",
    "$$\\min_{w, b, \\xi} \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "soggetto a:\n",
    "$$z_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad i = 1,\\ldots,n$$\n",
    "$$\\xi_i \\geq 0, \\quad i = 1,\\ldots,n$$\n",
    "\n",
    "### Cosa significa?\n",
    "\n",
    "1. **$\\frac{1}{2}\\|w\\|^2$**: Termine di **regolarizzazione**. Minimizzarlo significa massimizzare il margine tra le classi. Infatti:\n",
    "   - Il margine geometrico è $\\frac{1}{\\|w\\|}$\n",
    "   - Minimizzare $\\|w\\|^2$ massimizza il margine\n",
    "\n",
    "2. **$C\\sum_i \\xi_i$**: Termine di **penalità per gli errori**. Ogni $\\xi_i$ quantifica quanto una osservazione $i$ viola il margine duro:\n",
    "   - Se $\\xi_i = 0$: il punto è ben classificato (dentro il margine o oltre)\n",
    "   - Se $\\xi_i > 0$: il punto viola il vincolo (è dentro il margine o malclassificato)\n",
    "   - $C$ controlla il trade-off tra margine grande e pochi errori\n",
    "\n",
    "3. **Il vincolo**: $z_i(w^T x_i + b) \\geq 1 - \\xi_i$ richiede che:\n",
    "   - Se $z_i = +1$: vogliamo $w^T x_i + b \\geq 1 - \\xi_i$ (è dalla parte giusta con margine almeno 1)\n",
    "   - Se $z_i = -1$: vogliamo $w^T x_i + b \\leq -1 + \\xi_i$ (è dalla parte giusta con margine almeno 1)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. La Hinge Loss e la Primal Loss\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "La **hinge loss** è una funzione di perdita per il singolo campione:\n",
    "\n",
    "$$\\ell_i(w, b) = \\max(0, 1 - z_i(w^T x_i + b))$$\n",
    "\n",
    "### Cosa rappresenta?\n",
    "\n",
    "- Se $z_i(w^T x_i + b) \\geq 1$: il punto è ben classificato con margine $\\geq 1$, quindi $\\ell_i = 0$ (no perdita)\n",
    "- Se $z_i(w^T x_i + b) < 1$: il punto viola il margine duro, e la perdita è pari alla violazione\n",
    "\n",
    "Matematicamente: $\\ell_i(w,b) = \\xi_i$ nel problema primale!\n",
    "\n",
    "### Primal Loss (Primal Objective)\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{n} \\max(0, 1 - z_i(w^T x_i + b))$$\n",
    "\n",
    "Equivalente al problema primale (eliminando le variabili slack).\n",
    "\n",
    "Nel tuo codice:\n",
    "```python\n",
    "def primal_loss(w_cap, x, z, C):\n",
    "    D = compute_D(x)  # D = [x; 1] (aggiungi il bias come ultima riga)\n",
    "    margins = z * (w_cap.T @ D)  # z_i * (w^T x_i + b)\n",
    "    hinge_loss = np.maximum(0, 1 - margins)\n",
    "    return 0.5 * np.linalg.norm(w_cap)**2 + C * np.sum(hinge_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Il Problema Duale (Dual Problem)\n",
    "\n",
    "Il problema primale è difficile da risolvere direttamente (non è convesso rispetto a tutte le variabili per il vincolo). Si usa la **dualità Lagrangiana**.\n",
    "\n",
    "### Lagrangiano Aumentato\n",
    "\n",
    "$$L(w, b, \\xi, \\alpha, \\beta) = \\frac{1}{2}\\|w\\|^2 + C\\sum_i \\xi_i - \\sum_i \\alpha_i[z_i(w^T x_i + b) - 1 + \\xi_i] - \\sum_i \\beta_i \\xi_i$$\n",
    "\n",
    "dove $\\alpha_i, \\beta_i \\geq 0$ sono i moltiplicatori di Lagrange.\n",
    "\n",
    "### Condizioni KKT (otimalità)\n",
    "\n",
    "Derivando rispetto a $w, b, \\xi$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = w - \\sum_i \\alpha_i z_i x_i = 0 \\quad \\Rightarrow \\quad w = \\sum_i \\alpha_i z_i x_i$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\sum_i \\alpha_i z_i = 0 \\quad \\Rightarrow \\quad \\sum_i \\alpha_i z_i = 0$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i - \\beta_i = 0 \\quad \\Rightarrow \\quad \\alpha_i + \\beta_i = C$$\n",
    "\n",
    "### Problema Duale Senza Riformulazione\n",
    "\n",
    "$$\\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j z_i z_j (x_i^T x_j)$$\n",
    "\n",
    "soggetto a:\n",
    "$$\\sum_i \\alpha_i z_i = 0$$\n",
    "$$0 \\leq \\alpha_i \\leq C$$\n",
    "\n",
    "Questo è il **dual problem classico**, ma ha un **vincolo di uguaglianza** che è scomodo da gestire.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Riformulazione con $D$ (Come nel tuo codice)\n",
    "\n",
    "Nel tuo codice, usi una **riformulazione elegante** che **elimina il vincolo di uguaglianza**.\n",
    "\n",
    "### L'idea\n",
    "\n",
    "Augmenti i dati: $D_i = [x_i; 1] \\in \\mathbb{R}^{d+1}$ (concateni il bias come ultima riga).\n",
    "\n",
    "Allora: $w^T x_i + b = \\tilde{w}^T D_i$ dove $\\tilde{w} = [w; b] \\in \\mathbb{R}^{d+1}$\n",
    "\n",
    "### Nuova formulazione duale\n",
    "\n",
    "$$\\min_\\alpha \\frac{1}{2}\\alpha^T H \\alpha - \\mathbf{1}^T \\alpha$$\n",
    "\n",
    "dove $H$ è una matrice $(n \\times n)$ con elementi:\n",
    "$$H_{ij} = z_i z_j D_i^T D_j$$\n",
    "\n",
    "soggetto a:\n",
    "$$0 \\leq \\alpha_i \\leq C$$\n",
    "\n",
    "**NON c'è più il vincolo** $\\sum_i \\alpha_i z_i = 0$! È magicamente incorporato nella matrice $H$.\n",
    "\n",
    "### Perché funziona?\n",
    "\n",
    "La matrice $H$ contiene il prodotto $z_i z_j$. Questa struttura **forza automaticamente** il vincolo di uguaglianza durante l'ottimizzazione, senza doverlo esplicitamente imporre.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. La Dual Loss (Dual Objective)\n",
    "\n",
    "Nel tuo codice, risolvi:\n",
    "\n",
    "$$\\min_\\alpha L(\\alpha) = \\frac{1}{2}\\alpha^T H \\alpha - \\mathbf{1}^T \\alpha$$\n",
    "\n",
    "### Nel codice\n",
    "\n",
    "```python\n",
    "def find_L(alpha, H):\n",
    "    Ha = H @ vcol(alpha)\n",
    "    return 0.5 * (vrow(alpha) @ Ha).ravel() - alpha.sum()  # = 0.5 alpha^T H alpha - 1^T alpha\n",
    "```\n",
    "\n",
    "Questo è il **dual loss**.\n",
    "\n",
    "### Relazione con il primale\n",
    "\n",
    "Per la **strong duality** (una proprietà della SVM):\n",
    "\n",
    "$$-L(\\alpha^*) = J(w^*, b^*)$$\n",
    "\n",
    "dove $\\alpha^*$ è la soluzione duale ottima e $w^*, b^*$ è la soluzione primale ottima.\n",
    "\n",
    "Quindi:\n",
    "- **Dual Loss** = $-L(\\alpha)$ = valore della funzione obiettivo duale\n",
    "- **Primal Loss** = $J(w)$ = valore della funzione obiettivo primale\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Recuperare la soluzione primale dal duale\n",
    "\n",
    "Una volta trovato $\\alpha^*$, recuperi $w^*$ usando:\n",
    "\n",
    "$$w^* = \\sum_i \\alpha_i^* z_i D_i$$\n",
    "\n",
    "Nel tuo codice:\n",
    "```python\n",
    "w_star_cap = vcol((alpha_opt*z*D).sum(1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Duality Gap\n",
    "\n",
    "$$\\text{Gap} = J(w^*) - (-L(\\alpha^*))$$\n",
    "\n",
    "Nel caso ideale (ottimo raggiunto): **Gap = 0**.\n",
    "\n",
    "Se il gap è grande, significa:\n",
    "- L'ottimizzatore non ha convergito\n",
    "- O le tolleranze sono troppo alte\n",
    "\n",
    "Nel tuo codice:\n",
    "```python\n",
    "primal = primal_loss(w_cap, x, z, C)  # J(w)\n",
    "dual = -find_L(alpha, compute_H(x, z))  # -L(alpha)\n",
    "gap = primal - dual\n",
    "```\n",
    "\n",
    "Un gap piccolo (es. $< 10^{-3}$) indica una buona convergenza.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Il Ruolo di $C$\n",
    "\n",
    "- **$C \\to \\infty$**: Penalizzi molto gli errori → hard margin (non tolleri violazioni)\n",
    "- **$C \\to 0$**: Penalizzi poco gli errori → soft margin (tolleri più violazioni)\n",
    "- **$C$ intermedio**: Trade-off tra margine e errori di training\n",
    "\n",
    "Un $C$ troppo grande può causare overfitting; uno troppo piccolo causa underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Perché la Hinge Loss per $J_{cap}$?\n",
    "\n",
    "La **hinge loss** $\\max(0, 1 - z_i(w^T x_i + b))$ è scelta perché:\n",
    "\n",
    "1. **È convessa**: L'ottimizzazione è facile e globalmente ottima\n",
    "2. **Ha un'interpretazione geometrica**: Penalizza i punti dentro il margine\n",
    "3. **È una rilassazione dell'errore di classificazione**: \n",
    "   - Errore 0-1: $\\mathbb{1}[z_i(w^T x_i + b) \\leq 0]$ (non convesso)\n",
    "   - Hinge loss: $\\max(0, 1 - z_i(w^T x_i + b))$ (convesso, approssima l'errore 0-1)\n",
    "4. **Coincide con $\\xi_i$**: Nel problema primale, le variabili slack $\\xi_i$ sono esattamente le hinge loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
