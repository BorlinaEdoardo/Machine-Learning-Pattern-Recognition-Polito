{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b1148a",
   "metadata": {},
   "source": [
    "# boh roba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77d7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inclusion\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as linalg\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a0c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def load_iris():\n",
    "    D,L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "    return D, L\n",
    "\n",
    "def vcol(v):\n",
    "    return np.array(v).reshape(v.size, 1)\n",
    "\n",
    "def vrow(v):\n",
    "    return np.array(v).reshape(1, v.size)\n",
    "\n",
    "\n",
    "# from lab5\n",
    "def logpdf_GAU_ND(x, mu, C):\n",
    "    M = C.shape[0] # == C.shape[1] == mu.shape[0] == x.shape[0]\n",
    "    N = x.shape[1]\n",
    "    Y = np.zeros(N)  \n",
    "    for i in range(N):\n",
    "        (sign, log_det_C) = np.linalg.slogdet(C)\n",
    "        x_i = vcol(x[:,i])\n",
    "        Y[i] = (-M/2*np.log(2*np.pi) - 0.5*sign*log_det_C - 0.5*(x_i-mu).T@linalg.inv(C)@(x_i-mu)).item()\n",
    "    return vcol(Y)\n",
    "\n",
    "def loglikelyhood(x, mu, C):\n",
    "    Y = logpdf_GAU_ND(x, mu, C)\n",
    "    return Y.sum()\n",
    "\n",
    "# get score for gaussian distributed CV\n",
    "def gauss_score_matrix(D, Nc, mu, C):\n",
    "    # Score matrix\n",
    "    S = np.zeros((Nc,  D.shape[1]))\n",
    "\n",
    "    for c in range(Nc):\n",
    "        S[c,:] = np.exp(logpdf_GAU_ND(D, vcol(mu[:,c]), C[:,:,c]).T)  \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4134745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "def split(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    DTR = D[:, idxTrain]\n",
    "    DTE = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LTE = L[idxTest]\n",
    "    return (DTR, LTR), (DTE, LTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd56ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L = load_iris()\n",
    "(DTR, LTR), (DTE, LTE) = split(D, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd4fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0: [4.96129032 3.42903226 1.46451613 0.2483871 ]\n",
      "C_0:[[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "\n",
      "mu_1: [5.91212121 2.78484848 4.27272727 1.33939394]\n",
      "C_1:[[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "\n",
      "mu_2: [6.45555556 2.92777778 5.41944444 1.98888889]\n",
      "C_2:[[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max Likelihood parameters:\n",
    "Nc = np.unique(LTR).size\n",
    "mu = np.zeros((4,Nc))\n",
    "C = np.zeros((4, 4, Nc))\n",
    "l = np.zeros(Nc) \n",
    "for c in range(Nc):\n",
    "    DTR_c = DTR[:, LTR == c]\n",
    "    mu[:,c] = DTR_c.mean(axis=1)\n",
    "    C[:,:,c] = ((DTR_c - vcol(mu[:,c])) @ (DTR_c - vcol(mu[:,c])).T) / DTR_c.shape[1]\n",
    "    l[c] = np.exp(loglikelyhood(DTR_c, vcol(mu[:,c]), C[:,:,c]))\n",
    "    print(f\"mu_{c}: {mu[:,c]}\\nC_{c}:{C[:,:,c]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26498c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood:  [7.53645059e+10 5.02921185e-03 1.17632117e-13]\n"
     ]
    }
   ],
   "source": [
    "# likelihood:\n",
    "print(\"likelihood: \",l)\n",
    "\n",
    "# Score matrix\n",
    "S = np.zeros((Nc,  DTE.shape[1])) # 3 classes, 50 values \n",
    "\n",
    "for c in range(Nc):\n",
    "    S[c,:] = np.exp(logpdf_GAU_ND(DTE, vcol(mu[:,c]), C[:,:,c]).T)\n",
    "\n",
    "# print(f\"\\n Score matrix:\\n{S}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93819fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err:  3.1551066431862095e-12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SJoint = 1/3*S\n",
    "Sol_SJOINT = np.load(\"./SJoint_MVG.npy\")\n",
    "\n",
    "err = (np.absolute(SJoint-Sol_SJOINT)/Sol_SJOINT).sum(0).sum()\n",
    "print(\"err: \", err, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2b29f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "SMarginal = vrow(SJoint.sum(0))\n",
    "\n",
    "SPost = SJoint/SMarginal\n",
    "\n",
    "Pred = np.argmax(SPost, axis=0)\n",
    "print(Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230cd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4.0000000000000036%\n",
      "Accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "# Method 1 (Creative method)\n",
    "tmp = (Pred - LTE)\n",
    "tmp = tmp[tmp != 0]\n",
    "#print(tmp)\n",
    "wrong = (tmp/tmp).sum()\n",
    "err = wrong/LTE.size\n",
    "\n",
    "\n",
    "# Method 2\n",
    "correct = (Pred == LTE).sum()\n",
    "#print(correct)\n",
    "acc = correct/LTE.size\n",
    "\n",
    "print(f\"Error: {(1-acc)*100}%\\nAccuracy: {(acc)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839b9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do everithing using the log-values (I hate everithing)\n",
    "# Score matrix\n",
    "SLog = np.zeros((Nc,  DTE.shape[1])) # 3 classes, 50 values \n",
    "\n",
    "for c in range(Nc):\n",
    "    SLog[c,:] = (logpdf_GAU_ND(DTE, vcol(mu[:,c]), C[:,:,c]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3845b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err:  9.324424924327251e-14\n"
     ]
    }
   ],
   "source": [
    "LogSJoint = np.log(1/3)+SLog\n",
    "Sol_LogSJOINT = np.load(\"./logSJoint_MVG.npy\")\n",
    "\n",
    "err = np.absolute((LogSJoint-Sol_LogSJOINT)/Sol_LogSJOINT).sum(0).sum()\n",
    "print(\"err: \", err, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c25ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_l = vcol(np.argmax(LogSJoint.sum(1), axis=0)).item()\n",
    "i_l_sus = (np.argmax(LogSJoint, axis=1))\n",
    "l = vcol(LogSJoint[:,i_l])\n",
    "LogMarginal = np.log(np.exp(np.delete(LogSJoint, i_l, axis=1)).sum(axis=0)) # doesn't work :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98fa9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogPost = LogSJoint-np.log(np.exp(LogSJoint).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be461e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.7792891585087076e-12\n"
     ]
    }
   ],
   "source": [
    "sol = np.load(\"./logPosterior_MVG.npy\")\n",
    "\n",
    "err =((np.exp(LogPost)-np.exp(sol))/np.exp(sol)).sum()\n",
    "\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41b84a",
   "metadata": {},
   "source": [
    "# Naive Bayes Gaussian Classifier\n",
    "\n",
    "We now consider the Naive Bayes version of the classifier. As we have seen, the Naive Bayes version of the MVG is simply a Gaussian classifier where the covariance matrices are diagonal. The ML solution for the mean parameters is the same, whereas the ML solution for the covariance matrices is\n",
    "\n",
    "$$\n",
    " \\textnormal{diag}(\\Sigma^*_c) = \\text{diag} \\left[ \\frac{1}{N_c} \\sum_i (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T \\right]\n",
    "$$\n",
    "\n",
    "i.e., the diagonal of the ML solution for the MVG model. Implement the Naive Bayes classifier.\n",
    "\n",
    "**NOTE:** since the number of features is small, we can adapt the MVG code by simply zeroing the out-of-diagonal elements of the MVG ML solution. This can be done, for example, multiplying element-wise the MVG ML solution with the identity matrix. The rest of the code remains unchanged. If we have large dimensional data, it may be advisable to implement ad-hoc functions to work directly with just the diagonal of the covariance matrices (we wonâ€™t do this in this course).\n",
    "\n",
    "The accuracy for the Naive Bayes classifier should be again 96.0% for this dataset. The **Solution** folder contains all the intermediate results, both in the likelihood and in the log-likelihood domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7be2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBC_0:\n",
      "[[0.13140479 0.         0.         0.        ]\n",
      " [0.         0.16270552 0.         0.        ]\n",
      " [0.         0.         0.03583767 0.        ]\n",
      " [0.         0.         0.         0.0108845 ]]\n",
      "NBC_1:\n",
      "[[0.26470156 0.         0.         0.        ]\n",
      " [0.         0.10613407 0.         0.        ]\n",
      " [0.         0.         0.21955923 0.        ]\n",
      " [0.         0.         0.         0.03208448]]\n",
      "NBC_2:\n",
      "[[0.30080247 0.         0.         0.        ]\n",
      " [0.         0.08533951 0.         0.        ]\n",
      " [0.         0.         0.18434414 0.        ]\n",
      " [0.         0.         0.         0.0804321 ]]\n"
     ]
    }
   ],
   "source": [
    "# Note NB => Naive Bayes\n",
    "NBC = np.zeros((4,4,3))\n",
    "for i in range(3):\n",
    "    NBC[:,:,i]= C[:,:,i] * np.eye(4, 4)\n",
    "    print(f\"NBC_{i}:\\n{NBC[:,:,i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10d04ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate S and S marginal\n",
    "NGS = gauss_score_matrix(DTE, 3, mu, NBC)\n",
    "NGS_joint = 1/3*NGS\n",
    "\n",
    "NGS_marginal = vrow(NGS_joint.sum(0))\n",
    "\n",
    "NGS_Post = NGS_joint/NGS_marginal\n",
    "\n",
    "NGS_Pred = np.argmax(NGS_Post, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2691ee1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2,\n",
       "       0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0,\n",
       "       0, 0, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGS_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f9522f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with Naive Bayes Gaussian classifier: 96.0%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for Naive Bayes\n",
    "NB_acc = (NGS_Pred[NGS_Pred == LTE].shape[0])/LTE.shape[0]\n",
    "print(f\"accuracy with Naive Bayes Gaussian classifier: {NB_acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f7080",
   "metadata": {},
   "source": [
    "# Tied Covariance Gaussian Classifier\n",
    "\n",
    "We now consider the Tied covariance version of the classifier. In this case, the class covariance matrices are tied, with $\\Sigma_c = \\Sigma$. We have seen that the ML solution for the class means is again the same. The ML solution for the covariance matrix is given by the empirical within-class covariance matrix\n",
    "\n",
    "$$\n",
    "\\Sigma^* = \\frac{1}{N} \\sum_c \\sum_i (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T\n",
    "$$\n",
    "\n",
    "Compute the ML solution for the model. Remember that we have already computed within-class covariance matrices when we implemented LDA. Alternatively, we can observe that \n",
    "\n",
    "$$\n",
    "\\Sigma^* = \\frac{1}{N} \\sum_c N_c \\Sigma^*_c\n",
    "$$\n",
    "\n",
    "where $\\Sigma^*_c$ is the ML solution for class $c$ for the MVG classifier.\n",
    "\n",
    "You should obtain\n",
    "\n",
    "$$\n",
    "\\Sigma^* =\n",
    "\\begin{bmatrix}\n",
    "0.23637589 & 0.09525344 & 0.1364944 & 0.03614529 \\\\\n",
    "0.09525344 & 0.11618517 & 0.05768855 & 0.0357726 \\\\\n",
    "0.1364944 & 0.05768855 & 0.14992811 & 0.03764588 \\\\\n",
    "0.03614529 & 0.0357726 & 0.03764588 & 0.04291763 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The accuracy for the tied covariance classifier should be 98.0% for this dataset. Again, the **Solution** folder contains all the intermediate results, both in the likelihood and in the log-likelihood domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7adbfaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT:\n",
      "[[0.23637589 0.09525344 0.1364944  0.03614529]\n",
      " [0.09525344 0.11618517 0.05768855 0.0357726 ]\n",
      " [0.1364944  0.05768855 0.14992811 0.03746458]\n",
      " [0.03614529 0.0357726  0.03746458 0.04291763]]\n",
      "accuracy with tied Gaussian classifier: 98.0%\n"
     ]
    }
   ],
   "source": [
    "CT = np.zeros((4,4))\n",
    "for c in range(3):\n",
    "    DTR_c = DTR[:, LTR == c]\n",
    "    CT += C[:,:,c] * DTR_c.shape[1]\n",
    "\n",
    "CT /= DTR.shape[1]\n",
    "\n",
    "print(f\"CT:\\n{CT}\")\n",
    "CT_Tensor = np.zeros((4,4,3))\n",
    "for i in range(3):\n",
    "    CT_Tensor[:,:,i] = CT\n",
    "\n",
    "TS = gauss_score_matrix(DTE, 3, mu, CT_Tensor)\n",
    "TS_joint = 1/3*TS\n",
    "\n",
    "TS_marginal = vrow(TS_joint.sum(0))\n",
    "\n",
    "TS_Post = TS_joint/TS_marginal\n",
    "\n",
    "TS_Pred = np.argmax(TS_Post, axis=0)\n",
    "\n",
    "T_acc = (TS_Pred[TS_Pred == LTE].shape[0])/LTE.shape[0]\n",
    "print(f\"accuracy with tied Gaussian classifier: {T_acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b80ed",
   "metadata": {},
   "source": [
    "## Binary task: log-likelihood ratios and MVG\n",
    "We now focus on the same binary task we employed for LDA (see Laboratory 3), which requries classi-\n",
    "fying only two kinds of flowers, iris versicolor and iris virginica. You can refer to Laboratory 2 to build\n",
    "the 2-class dataset.\n",
    "\n",
    "Although we could proceed in the same way as for the multiclass iris problem, for binary tasks we have\n",
    "seen that we can cast the classification as a comparison of a score, the log-likelihood ratio, with a thresh-\n",
    "old t that depends on class priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07b4cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris_sklearn():\n",
    "    return sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "DIris, LIris = load_iris_sklearn()\n",
    "D = DIris[:, LIris != 0]\n",
    "L = LIris[LIris != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c7744bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    DTR = D[:, idxTrain]\n",
    "    DVAL = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LVAL = L[idxTest]\n",
    "    return (DTR, LTR), (DVAL, LVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d99ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTR and LTR are model training data and labels\n",
    "# DVAL and LVAL are validation data and labels\n",
    "(DTR, LTR), (DVAL, LVAL) = split_db_2to1(D, L)\n",
    "\n",
    "# we assume that classe 2 is true\n",
    "true_class = 2\n",
    "# and 1 is false\n",
    "false_class = 1\n",
    "\n",
    "# mu and C computed with max likelihood method\n",
    "mu_1 = DTR[:, LTR == 1].mean(axis=1)\n",
    "mu_2 = DTR[:, LTR == 2].mean(axis=1)\n",
    "\n",
    "C_1 = ((DTR[:, LTR == 1] - vcol(mu_1)) @ (DTR[:, LTR == 1] - vcol(mu_1)).T)/DTR[:, LTR == 1].shape[1]\n",
    "C_2 = ((DTR[:, LTR == 2] - vcol(mu_2)) @ (DTR[:, LTR == 2] - vcol(mu_2)).T)/DTR[:, LTR == 1].shape[1]\n",
    "\n",
    "# score computations\n",
    "Score = np.zeros((2, DVAL.shape[1]))\n",
    "\n",
    "Score[0,:] = vrow(logpdf_GAU_ND(DVAL, vcol(mu_1), C_1))\n",
    "Score[1,:] = vrow(logpdf_GAU_ND(DVAL, vcol(mu_2), C_2))\n",
    "\n",
    "llr = Score[1,:] - Score[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d84bcc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12.17161321,   5.62521909, -12.01169368,   8.25893696,\n",
       "       -11.59666694, -17.36267605,  -9.07501906,   6.98399727,\n",
       "        15.17586183,   0.68668463,   1.10299881,  22.67572956,\n",
       "        12.97330375,   2.10622477, -10.79680446,  -8.15595229,\n",
       "        -1.0132232 ,  -8.83632766,  -7.76493548,  17.21681417,\n",
       "         7.31573716, -12.8663077 ,   2.32407629,  14.26593151,\n",
       "         2.39028403,  -8.15262229, -11.90205646,  -1.37772105,\n",
       "        -5.20559802,   7.04936964,   3.31844988,  28.86231038,\n",
       "       -10.43354846,  -7.41289129])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cda46249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative % classification error = 8.823529411764707%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A priori probability for class 1 and class 2 are the same => threshold = 0\n",
    "L_pred = np.zeros(LVAL.shape[0])\n",
    "i = 0\n",
    "for p in llr:\n",
    "    L_pred[i] = 1 if p < 0 else 2\n",
    "    i += 1\n",
    "\n",
    "rel_class_err = np.abs(L_pred-LVAL).sum()/LVAL.shape[0]\n",
    "print(f\"Relative % classification error = {rel_class_err*100}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c0630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
